---
layout: post
title:  "Matthews Correlation Coefficient and other common Statistical measurements"
tags: [Coding, Bioinformatics, Algorithms, Statistics]
---

> A measure of the quality of binary (two-class) classifications, 
introduced by biochemist Brian W. Matthews in 1975. It takes 
into account true and false positives and negatives and is generally 
regarded as a balanced measure which can be used even if the 
classes are of very different sizes. The MCC is in essence 
a correlation coefficient between the observed and predicted 
binary classifications; it returns a value between −1 and +1. 
A coefficient of +1 represents a perfect prediction, 0 no better 
than random prediction and −1 indicates total disagreement between 
prediction and observation. The statistic is also known as the 
phi coefficient. MCC is related to the chi-square statistic for 
a 2×2 contingency table


#### Confusion Matrix
*[Table 1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5721660/#Sec9)*

![image](https://user-images.githubusercontent.com/5554422/45764534-ea327f80-bc00-11e8-8463-ff323fcc579f.png)

Based on the confusion matrix, the goal of a classifier is to maximize **TP** and **TN**. A simple way to do this is 
to use **accuracy** or **F1 score**

$$ accuracy = \frac{TP+TN}{TP+TN+FP+FN} ; F1 = \frac{2 \times TP}{2 \times TP+FP+FN} $$

However both of these scores can be misleading. For example if the predictor predicts 
only positive outcomes and you will not be able to tell the mistake by just looking at 
accuracy or F1 score. 

Hence people use MCC to overcome this effect:

**MCC** $$ = \frac{TP\times TN-FP\times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$

**Precision** $$ = \frac{TP}{TP+FP}$$: How many true positive predictions in all the positive predictions.
The probability that a (randomly selected) retrieved document is relevant.

**Recall/Sensitivity** $$ = \frac{TP}{TP+FN} $$: How many true positive predictions in all the positive conditions.
The probability that a (randomly selected) relevant document is retrieved in a search.

**Specificity** $$ = \frac{TN}{TN +FP} $$: How many true negative predictions in all the negative conditions

In this case, the a measure that balances prediction sensitivity 
and specificity. MCC ranges from − 1, indicating an inverse 
prediction, through 0, which corresponds to a random classifier, 
to 1 for perfect prediction.

#### Example for MCC 

(sudo code)

```
 actual = [0, 1, 1, 0, 0, 1]
 predicted = [0, 1, 0, 1, 1, 1]
 
 for i in predicted:
    # build confusion matrix
    # calculate MCC
 
```

About the PR-Curve

The precision-recall curve shows the tradeoff between precision 
and recall for different threshold. A high area under the curve 
represents both high recall and high precision, where high precision 
relates to a low false positive rate, and high recall relates to 
a low false negative rate. High scores for both show that the 
classifier is returning accurate results (high precision), as well 
as returning a majority of all positive results (high recall).

A system with high recall but low precision returns many results, 
but most of its predicted labels are incorrect when compared to 
the training labels. A system with high precision but low recall 
is just the opposite, returning very few results, but most of its 
predicted labels are correct when compared to the training labels. 
An ideal system with high precision and high recall will return 
many results, with all results labeled correctly.


**Average Precison (AP)**

Limited to classification tasks

Summarizes such a plot as the weighted mean of precisions 
achieved at each threshold, with the increase in recall from 
the previous threshold used as the weight

$$ AP = \sum_{n}(R_{n} R_{n-1})P_{n}$$

Where $$P_{n}$$ and $$R_{n}$$ are the precision and recall at the 
nth threshold.


#### Example for Precision Recall

```
import numpy as np
from sklearn.metrics import precision_recall_curve

y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
```




