---
layout: post
title:  "Chapter 3"
tags: [Deep Learning, Machine Learning, Notes]
---

##### Notes on the [Deep Learning Textbook](http://www.deeplearningbook.org/) Chapter 3 - Probability and Information Theory
---

#### Distributions
* 
* [Multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)
* [Empirical distribution](https://www.statlect.com/asymptotic-theory/empirical-distribution)
* [Gaussian mixture model](http://research.stowers.org/mcm/efg/R/Statistics/MixturesOfDistributions/index.html), also more on [Scikit](http://scikit-learn.org/stable/modules/mixture.html)

---

#### Useful Functions

* Logistics sigmoid: commonly used to produce the phi parameter for beunoulli distribution 
* Softplus: mean and variance for nomal distribution 
* Information theory
* Self information: 
* Shannon entropy
* Kullback-Leibler (KL) divergence: measuring the same random variable in two different distributions [read more](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
* Cross entropy
* Structured probabilistic mode, or graphical model 
